# -*- coding: utf-8 -*-
"""Gemini Pro with Langchain_SINDHUNAGESHA_017419987 .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G6jcWZEav3CcPq9LW-1Sru4D4kis8v46

# Gemini Pro with LangChain on Google AI
"""

!pip -q install langchain_experimental langchain_core
!pip -q install google-generativeai==0.3.1
!pip -q install google-ai-generativelanguage==0.4.0
!pip -q install langchain-google-genai
!pip -q install "langchain[docarray]"

!pip show langchain langchain-core

#@title Setting up the Auth
import os
import google.generativeai as genai
from google.colab import userdata

from IPython.display import display
from IPython.display import Markdown

os.environ["GOOGLE_API_KEY"] ='AIzaSyAzgTxl_aXUu6822KPENkZmwu9ExmNJQZI'

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

models = [m for m in genai.list_models()]
models

# generate text
prompt = 'Who are you and what can you do?'

model = genai.GenerativeModel('gemini-pro')

response = model.generate_content(prompt)

Markdown(response.text)

prompt = 'Explain the purpose of the `SparkConf` class in Apache Spark?'

model = genai.GenerativeModel('gemini-pro')

response = model.generate_content(prompt)

Markdown(response.text)

"""# Google's Gemini with LangChain


## Basic LLM Chain

"""

from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI


llm = ChatGoogleGenerativeAI(model="gemini-pro",
                             temperature=0.7)


result = llm.invoke("What is a LLM?")

Markdown(result.content)

for chunk in llm.stream("Write a haiku about LLMs."):
    print(chunk.content)
    print("---")

"""## Basic Multi Chain - Interaction with Google AI"""

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser


model = ChatGoogleGenerativeAI(model="gemini-pro",
                             temperature=0.7)

prompt = ChatPromptTemplate.from_template(
    "tell me a short joke about {topic}"
)

output_parser = StrOutputParser()

chain = prompt | model | output_parser

chain.invoke({"topic": "machine learning"})

"""## More Complicated Chain - Mini RAG - Similar Text Identification"""

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.vectorstores import DocArrayInMemorySearch

model = ChatGoogleGenerativeAI(model="gemini-pro",
                             temperature=0.7)

pip install "langchain[docarray]"

pip show pydantic

pip install --upgrade pydantic

import tensorflow_hub as hub
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Load the Universal Sentence Encoder module
use_model_url = "https://tfhub.dev/google/universal-sentence-encoder/4"
embed = hub.load(use_model_url)

# Define your text data
texts = [
    "Gemini Pro is a Large Language Model was made by GoogleDeepMind",
    "Gemini can be either a star sign or a name of a series of language models",
    "A Language model is trained by predicting the next token",
    "LLMs can easily do a variety of NLP tasks as well as text generation"
]

# Obtain embeddings for the text data
embeddings = embed(texts)

# Convert embeddings to numpy arrays
embeddings_np = np.array(embeddings)

# Define a query text
query_text = "Gemini is a language model developed by Google."

# Obtain embedding for the query text
query_embedding = embed([query_text])[0]

# Calculate cosine similarity between query embedding and all other embeddings
similarities = cosine_similarity([query_embedding], embeddings_np)

# Find index of the most similar text
most_similar_index = np.argmax(similarities)

# Retrieve the most similar text
most_similar_text = texts[most_similar_index]

print("Most similar text:", most_similar_text)

# Define a query text
query_text = "What is Gemini?"

# Obtain embedding for the query text
query_embedding = embed([query_text])[0]

# Calculate cosine similarity between query embedding and all other embeddings
similarities = cosine_similarity([query_embedding], embeddings_np)

# Get indices of relevant documents based on similarity scores
relevant_indices = np.argsort(similarities[0])[::-1]

# Define a threshold for similarity score
threshold = 0.5  # Adjust as needed

# Filter relevant documents based on threshold
relevant_documents = [texts[i] for i in relevant_indices if similarities[0][i] > threshold]

print("Relevant documents:")
for document in relevant_documents:
    print("-", document)

"""## PAL Chain - Math"""

from langchain_experimental.pal_chain import PALChain

from langchain.chains.llm import LLMChain

model = ChatGoogleGenerativeAI(model="gemini-pro",
                             temperature=0)

pal_chain = PALChain.from_math_prompt(model, verbose=True)

question = "The cafeteria had 23 apples. \
If they used 20 for lunch and bought 6 more,\
how many apples do they have?"

pal_chain.invoke(question)

question = "If you wake up at 7:00 a.m. and it takes you 1 hour and 30 minutes to get ready \
 and walk to school, at what time will you get to school?"

pal_chain.invoke(question)

"""## Multi Modal- Image and explanation"""

import requests
from IPython.display import Image

image_url = "https://upload.wikimedia.org/wikipedia/commons/thumb/9/97/The_Earth_seen_from_Apollo_17.jpg/1200px-The_Earth_seen_from_Apollo_17.jpg"
content = requests.get(image_url).content
Image(content,width=300)

from langchain_core.messages import HumanMessage
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(model="gemini-pro-vision")

# example
message = HumanMessage(
    content=[
        {
            "type": "text",
            "text": "What's in this image and who lives there?",
        },  # You can optionally provide text parts
        {
            "type": "image_url",
            "image_url": image_url
         },
    ]
)

llm.invoke([message])

"""Tokenization"""

import nltk
import spacy

# Download necessary resources
nltk.download('punkt')

# Initialize spaCy English model
nlp = spacy.load("en_core_web_sm")

def process_query(query):

    # Tokenization
    tokens = nltk.word_tokenize(query.lower())

    # Named Entity Recognition (NER)
    doc = nlp(query)
    entities = [(ent.text, ent.label_) for ent in doc.ents]

    # Basic relation extraction (verb - entity)
    relations = {}
    for token in doc:
        if token.pos_ == "VERB":
            for ent in entities:
                # Simple relation: verb -> entity (can be improved)
                relations[token.text] = relations.get(token.text, []) + [ent]

    return {
        "tokens": tokens,
    }

# Example usage
query = "Can you write a poem about space exploration?"
processed_query = process_query(query)

print("Tokens:", processed_query["tokens"])

"""###Querying and storing it in json format."""

pip install -q -U google-generativeai

import os
import google.generativeai as genai

os.environ["GOOGLE_API_KEY"] ='AIzaSyAzgTxl_aXUu6822KPENkZmwu9ExmNJQZI'

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])
import google.generativeai as genai

model = genai.GenerativeModel('gemini-pro')

prompt='''
1.You are a Named Entity Recognition in Indonesian Language.
2.Do some analysis to extract the Entity from the text for some categories, i.e., Person, Organization, Location, Date/Time, and other as Miscellaneous.
3.Output Person category as PER, Organization category as ORG, Location category as LOC, Date/Time category as DT, and Miscellaneous category as MISC.
4.Return this result as JSON for each entity with character offset from each result.
Analyze the sentences as follow: "'
'''

query = "ISTTS sebagai perguruan tinggi di Indonesia terletak di Surabaya"
response = model.generate_content(prompt+query +'"')
print(response.text)

import json
# Saving response to a JSON file
output_data = {
    "query": query,
    "response": response.text.strip()
}

with open('output.json', 'w') as json_file:
    json.dump(output_data, json_file, indent=4)

print("Output has been saved to output.json.")

prompt='''
1.Do some analysis to extract the Entity from the text for some categories, i.e., Person, Organization, Location, Date/Time, and other as Miscellaneous.
2.Output Person category as PER, Organization category as ORG, Location category as LOC, Date/Time category as DT, and Miscellaneous category as MISC.
3.Provide a relation extraction from those entity for build the knowledge graph.
4.Provide this NER result consist of each entity with character offset and the relation extraction result as edge list.
Analyze the input and produce the output as JSON as follow: "'
'''

query = 'Indonesia dengan nama resmi Republik Indonesia adalah sebuah negara kepulauan di Asia Tenggara yang dilintasi garis khatulistiwa dan berada di antara daratan benua Asia dan Oseania sehingga dikenal sebagai negara lintas benua, serta antara Samudra Pasifik dan Samudra Hindia.'
response1 = model.generate_content(prompt+query +'"')
print(response1.text)

import json
# Saving response to a JSON file
output_data = {
    "query": query,
    "response1": response1.text.strip()
}

with open('output1.json', 'w') as json_file:
    json.dump(output_data, json_file, indent=4)

print("Output has been saved to output1.json.")

"""SQL with llm"""

!pip install -q google-generativeai==0.3.1

import google.generativeai as genai
from pathlib import Path
import sqlite3

genai.__version__

!pip install -q langchain

!wget https://bafybeibqw3muonedrl57jvmhh2oemftjvamtc22htygi7vl52qmgbvqmk4.ipfs.nftstorage.link/retailDB.sqlite -O retail.sqlite

os.environ["GOOGLE_API_KEY"] ='AIzaSyAzgTxl_aXUu6822KPENkZmwu9ExmNJQZI'

genai.configure(api_key=os.environ["GOOGLE_API_KEY"])

generation_config = {
  "temperature": 0.4,
  "top_p": 1,
  "top_k": 32,
  "max_output_tokens": 4096,
}

safety_settings = [
  {
    "category": "HARM_CATEGORY_HARASSMENT",
    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
  },
  {
    "category": "HARM_CATEGORY_HATE_SPEECH",
    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
  },
  {
    "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
  },
  {
    "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
    "threshold": "BLOCK_MEDIUM_AND_ABOVE"
  }
]

model = genai.GenerativeModel(model_name = "gemini-pro",
                              generation_config = generation_config,
                              safety_settings = safety_settings)

def read_sql_query(sql, db):
    conn = sqlite3.connect(db)
    cur = conn.cursor()
    cur.execute(sql)
    rows = cur.fetchall()
    for row in rows:
        print(row)
    conn.close()

read_sql_query('SELECT * FROM brands LIMIT 10;',
               "retail.sqlite")

prompt_parts_1 = [
  "You are an expert in converting English questions to SQL code!The SQL database has the name retails in which it has table called brands which  has the following columns -  product_id,brand, and modified_brand write sql code to retrive the product_id of brand Adidas;Dont include ``` and \n in the output",
]

question = "write sql code To determine the brand with least discount?"

prompt_parts = [prompt_parts_1[0], question]
response2 = model.generate_content(prompt_parts)
response2.text

read_sql_query("""SELECT revenue FROM finance WHERE product_id = 'G27341' AND discount = (SELECT MAX(discount) FROM finance WHERE product_id = 'G27341');
""",
               "retail.sqlite")

def generate_gemini_response(question, input_prompt):
    prompt_parts = [input_prompt, question]
    response = model.generate_content(prompt_parts)
    output = read_sql_query(response2.text, "fretail.sqlite")
    return output

"""summarization

"""

!pip install transformers langchain

from langchain import HuggingFacePipeline

model_id = "facebook/bart-large-cnn"
task = "summarization"

llm = HuggingFacePipeline.from_model_id(model_id=model_id, task=task)

from langchain import PromptTemplate, LLMChain

template = """{input}"""
prompt = PromptTemplate(template=template, input_variables=["input"])

llm_chain = LLMChain(prompt=prompt, llm=llm)

question = """
The U.S. will be looking to snag their third straight World Cup title — and its fifth overall.

The U.S. women's national team (USWNT) has held the No. 1 spot in FIFA's rankings for years, and is the odds-on favorite to win once again. But this year's tournament is considered fairly wide open, with several teams having a decent shot at the title.

Indeed, the U.S. has had an interesting pattern of late: winning the World Cup, but losing at the Olympics. The USWNT won the World Cup in 2015, then failed to medal at the 2016 Olympics. They won the 2019 World Cup, then took home the bronze at the Tokyo Olympics in 2021.
"""
answer = llm_chain.run(question)

answer

len(answer)

len(question)

from langchain import PromptTemplate
template = """
你精通多种语言，是专业的翻译官。你负责{src_lang}到{dst_lang}的翻译工作。
"""

prompt = PromptTemplate.from_template(template)
prompt.format(src_lang="英文", dst_lang="中文")

from langchain.prompts import (
    ChatPromptTemplate,
    PromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

template="You are a helpful assistant that translates {input_language} to {output_language}."
system_message_prompt = SystemMessagePromptTemplate.from_template(template)
system_message_prompt.format(input_language="English", output_language="Japanese")

system_template="You are a professional translator that translates {src_lang} to {dst_lang}."
system_message_prompt = SystemMessagePromptTemplate.from_template(system_template)

human_template="{user_input}"
human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)

chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])
chat_prompt.format_prompt(
    src_lang="English",
    dst_lang="Chinese",
    user_input="Did you eat in this morning?"
).to_messages()

from langchain.prompts import PromptTemplate
from langchain.prompts import FewShotPromptTemplate
from langchain.prompts.example_selector import LengthBasedExampleSelector


examples = [
    {"input": "happy", "output": "sad"},
    {"input": "tall", "output": "short"},
    {"input": "energetic", "output": "lethargic"},
    {"input": "sunny", "output": "gloomy"},
    {"input": "windy", "output": "calm"},
]
example_prompt = PromptTemplate(
    input_variables=["input", "output"],
    template="Input: {input}\nOutput: {output}",
)
example_selector = LengthBasedExampleSelector(
    # 可选的样本数据
    examples=examples,
    # 提示词模版
    example_prompt=example_prompt,
    # 格式化的样本数据的最大长度，通过get_text_length函数来衡量
    max_length=25,
    # get_text_length: ...
)
dynamic_prompt = FewShotPromptTemplate(
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix="Give the antonym of every input",
    suffix="Input: {adjective}\nOutput:",
    input_variables=["adjective"],
)

dynamic_prompt.format(adjective="big")

import requests
import google.generativeai as genai

def get_repo_stats(repo_url):
    """
    Function to get statistics for a given GitHub repository.
    """
    # Extracting owner and repo name from the URL
    parts = repo_url.split('/')
    owner = parts[-2]
    repo_name = parts[-1]

    # Constructing the GitHub API URL
    api_url = f"https://api.github.com/repos/{owner}/{repo_name}"

    # Sending a GET request to the GitHub API
    response = requests.get(api_url)

    if response.status_code == 200:
        repo_data = response.json()
        stars = repo_data['stargazers_count']
        forks = repo_data['forks_count']
        issues = repo_data['open_issues_count']
        return stars, forks, issues
    else:
        return None, None, None

def interact_with_codebase():
    """
    Function to interact with a codebase and provide insights.
    """
    repo_url = "https://github.com/apache/spark"

    stars, forks, issues = get_repo_stats(repo_url)
    if stars is not None:
        print(f"The Apache Spark repository has {stars} stars, {forks} forks, and {issues} open issues.")


    else:
        print("Failed to fetch repository statistics. Please check the URL and try again.")


if __name__ == "__main__":
    interact_with_codebase()
prompt = 'Explain the purpose of the `SparkConf` class in Apache Spark?'
model = genai.GenerativeModel('gemini-pro')
response = model.generate_content(prompt)
Markdown(response.text)

